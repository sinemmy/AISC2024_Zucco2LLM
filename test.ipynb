{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#from src.load_zuco_data import *\n",
    "import numpy as np\n",
    "import torch \n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from src.load_zuco_sentences import *\n",
    "\n",
    "# tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "# transformer lens\n",
    "import tqdm.auto as tqdm\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import HookedTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD SENTENCES from ZUCO data and SAVE into a CSV \n",
    "Do not need to re-run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences into a data frame \n",
    "sentence_data_file = '../zuco-benchmark/portable_data/sentence_content.json'\n",
    "# this file has all the sentences in the dataset seen by each subject\n",
    "df_sentences = load_zuco_dataframe(sentence_data_file)\n",
    "# we don't need the subject data, we just need the sentences - which will have unique indices for a task\n",
    "df2 = df_sentences[['task', 'index', 'sentence']].drop_duplicates(subset=['task', 'index'])\n",
    "# \n",
    "unique_sentences = df2.drop_duplicates(subset=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>TSR</td>\n",
       "      <td>379</td>\n",
       "      <td>In the 40s, Gillespie led the movement called ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>TSR</td>\n",
       "      <td>380</td>\n",
       "      <td>In 1867, his brother's company, Rockefeller &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>TSR</td>\n",
       "      <td>381</td>\n",
       "      <td>Married to Almira Geraldine Goodsell, he built...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>TSR</td>\n",
       "      <td>382</td>\n",
       "      <td>Libby was a founding member of the Project for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>TSR</td>\n",
       "      <td>383</td>\n",
       "      <td>He was elected to the Bulgarian national assem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>TSR</td>\n",
       "      <td>384</td>\n",
       "      <td>He also created the Defense Intelligence Agenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>TSR</td>\n",
       "      <td>385</td>\n",
       "      <td>He was one of the founder members of the Lunar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>TSR</td>\n",
       "      <td>387</td>\n",
       "      <td>He was the founder and first president of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>TSR</td>\n",
       "      <td>388</td>\n",
       "      <td>Her mother was a Lyman, another very old Ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>TSR</td>\n",
       "      <td>389</td>\n",
       "      <td>In 1999 Bush cofounded a educational-software ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    task  index                                           sentence\n",
       "728  TSR    379  In the 40s, Gillespie led the movement called ...\n",
       "729  TSR    380  In 1867, his brother's company, Rockefeller & ...\n",
       "730  TSR    381  Married to Almira Geraldine Goodsell, he built...\n",
       "731  TSR    382  Libby was a founding member of the Project for...\n",
       "732  TSR    383  He was elected to the Bulgarian national assem...\n",
       "733  TSR    384  He also created the Defense Intelligence Agenc...\n",
       "734  TSR    385  He was one of the founder members of the Lunar...\n",
       "736  TSR    387  He was the founder and first president of the ...\n",
       "737  TSR    388  Her mother was a Lyman, another very old Ameri...\n",
       "738  TSR    389  In 1999 Bush cofounded a educational-software ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sentences.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 sentences that appear in both NR and TSR tasks:\n",
      "- His wife is Barbara Bush.\n",
      "- In 1962, Clampett created an animated version of the show called Beany and Cecil, which ran on ABC for five years.\n",
      "- He was, for a short time, a commentator opposite Bill Clinton on CBS's 60 Minutes.\n",
      "- He was married to actress Kim Basinger from 1993 to 2002.\n",
      "- In 1966 she went to United Artists Records.\n",
      "- Finally, toward the end of 1958, she signed with RCA Victor Records, where she stayed until 1963 except for doing some recordings in 1960 for Reprise Records.\n",
      "- Franklin James Schaffner (May 30, 1920 – July 2, 1989) was an American film director.\n",
      "- That year, he also married Rose Fitzgerald, the daughter of John F. Fitzgerald, the Democrat mayor of Boston and probably the most recognized politician in the city.\n",
      "- George David Birkhoff (21 March 1884 - 12 November 1944) was an American mathematician, and one of the most important leaders in mathematics in the USA in his generation.\n",
      "- Erasmus Darwin (December 12, 1731 – April 18, 1802) trained as a physician and wrote extensively on medicine and botany, as well as poetry.\n",
      "...and 46 more.\n"
     ]
    }
   ],
   "source": [
    "#let's check if there are duplicate sentences across tasks\n",
    "\n",
    "# Get set of sentences in each task\n",
    "nr_sentences = set(df2[df2['task'] == 'NR']['sentence'])\n",
    "tsr_sentences = set(df2[df2['task'] == 'TSR']['sentence'])\n",
    "\n",
    "# Find intersection\n",
    "common_sentences = nr_sentences.intersection(tsr_sentences)\n",
    "\n",
    "if common_sentences:\n",
    "    print(f\"Found {len(common_sentences)} sentences that appear in both NR and TSR tasks:\")\n",
    "    for sentence in list(common_sentences)[:10]:  # Show first 10 as example\n",
    "        print(f\"- {sentence}\")\n",
    "    if len(common_sentences) > 10:\n",
    "        print(f\"...and {len(common_sentences) - 10} more.\")\n",
    "else:\n",
    "    print(\"No sentences appear in both tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title save csv (note that index = -100 if the sentence is not present in one of the tasks)\n",
    "\n",
    "# We want to get the senteces out for an LLM, so we only need the unique sentences. \n",
    "# We should however, keep track of the sentence index and which task(s) it was used in \n",
    "# We can do this by merging the two dataframes on the sentence column, and keeping the index from each task\n",
    "# We will also fill in the index with -100 for sentences that are not present in one of the tasks (in order to avoid nans)\n",
    "\n",
    "\n",
    "# Create two separate DataFrames for each task, dropping duplicates first\n",
    "nr_df = df2[df2['task'] == 'NR'][['index', 'sentence']].drop_duplicates(subset=['sentence']).rename(columns={'index': 'NR_index'})\n",
    "tsr_df = df2[df2['task'] == 'TSR'][['index', 'sentence']].drop_duplicates(subset=['sentence']).rename(columns={'index': 'TSR_index'})\n",
    "\n",
    "# Merge the DataFrames on the sentence, using outer join to keep all sentences\n",
    "result_df = pd.merge(nr_df, tsr_df, on='sentence', how='outer')\n",
    "\n",
    "# Fill NaN values with -100\n",
    "result_df = result_df.fillna(-100)\n",
    "\n",
    "# Convert indices to integers\n",
    "result_df['NR_index'] = result_df['NR_index'].astype(int)\n",
    "result_df['TSR_index'] = result_df['TSR_index'].astype(int)\n",
    "\n",
    "# Sort by sentence for easier reading\n",
    "result_df = result_df.sort_values('sentence').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unique sentences with task indices to a CSV file\n",
    "csv_path = 'zuco_unique_sentences_with_task_indices.csv'\n",
    "result_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV to DATALOADER\n",
    "\n",
    "We are going to need to tokenize. For transformer lens, we can see the models which are available in the [model properties table](https://transformerlensorg.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "\n",
    "Let's start with GPT2-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "csv_path = 'zuco_unique_sentences_with_task_indices.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'zuco_unique_sentences_with_task_indices.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m tokenizer.pad_token = tokenizer.eos_token \n\u001b[32m      5\u001b[39m transform = TokenizerTransform(tokenizer) \n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m dataloader = \u001b[43mget_zuco_sentence_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/AISC2024_Zucco2LLM/src/load_zuco_sentences.py:97\u001b[39m, in \u001b[36mget_zuco_sentence_dataloader\u001b[39m\u001b[34m(csv_path, batch_size, shuffle, transform)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenizer):\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenizer = tokenizer\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample):\n\u001b[32m     98\u001b[39m     \u001b[38;5;66;03m# Tokenize the sentence\u001b[39;00m\n\u001b[32m     99\u001b[39m     tokenized = \u001b[38;5;28mself\u001b[39m.tokenizer(\n\u001b[32m    100\u001b[39m         sample[\u001b[33m'\u001b[39m\u001b[33msentence\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    101\u001b[39m         return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m         max_length=\u001b[32m128\u001b[39m\n\u001b[32m    105\u001b[39m     )\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# Remove the batch dimension\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/AISC2024_Zucco2LLM/src/load_zuco_sentences.py:54\u001b[39m, in \u001b[36mZucoSentenceDataset.__init__\u001b[39m\u001b[34m(self, csv_path, transform)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_path, transform=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     44\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[33;03m    Initialize the dataset from a CSV file\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m \u001b[33;03m        Optional transform to be applied to each sample\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mself\u001b[39m.transform = transform\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/AISC2024_Zucco2LLM/.conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/AISC2024_Zucco2LLM/.conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/AISC2024_Zucco2LLM/.conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/AISC2024_Zucco2LLM/.conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/AISC2024_Zucco2LLM/.conda/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'zuco_unique_sentences_with_task_indices.csv'"
     ]
    }
   ],
   "source": [
    "# GPT2-medium data loader \n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "transform = TokenizerTransform(tokenizer) \n",
    "\n",
    "dataloader = get_zuco_sentence_dataloader(\n",
    "    csv_path=csv_path,\n",
    "    transform=transform,\n",
    "    batch_size=15,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2-medium with transformer lens- collect activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x14ba5cfd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False) # don't need gradients for this (not training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of 1 sentences\n",
      "Number in NR task: 1\n",
      "NR indices: tensor([256])\n",
      "TSR indices: tensor([-100])\n",
      "Number in TSR task: 0\n",
      "Sample sentence: He soon recorded Desireless, and it became a hit across Scandinavia and the rest of Europe, and an American release came in 1998.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a dataloader\n",
    "# dataloader = get_zuco_sentence_dataloader(\n",
    "#     csv_path=csv_path,\n",
    "    \n",
    "# )\n",
    "\n",
    "# Example of iterating through the dataloader\n",
    "for batch in dataloader:\n",
    "    sentences = batch['sentence']\n",
    "    in_nr = batch['in_NR']\n",
    "    in_tsr = batch['in_TSR']\n",
    "    NR_indices = batch['NR_index']\n",
    "    TSR_indices = batch['TSR_index']\n",
    "   # Your model processing here...\n",
    "    # ...\n",
    "    \n",
    "    # Just for demonstration\n",
    "    print(f\"Batch of {len(sentences)} sentences\")\n",
    "    print(f\"Number in NR task: {in_nr.sum().item()}\")\n",
    "    print(f\"NR indices: {NR_indices}\")\n",
    "    print(f\"Number in TSR task: {in_tsr.sum().item()}\")\n",
    "    print(f\"TSR indices: {TSR_indices}\")\n",
    "\n",
    "    print(f\"Sample sentence: {sentences[0]}\")\n",
    "    break  # Just show one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC (unused/testing code - can ignore this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbatch\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'batch' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by task and index\n",
    "all_sentences_df = all_sentences_df.sort_values(by=['task', 'index'])\n",
    "\n",
    "# Now you can easily filter by task, subject, get unique sentences, etc.\n",
    "nr_sentences = all_sentences_df[all_sentences_df['task'] == 'NR']\n",
    "subject_tsr_sentences = all_sentences_df[(all_sentences_df['task'] == 'TSR') & \n",
    "                                        (all_sentences_df['subject'] == 'YAC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ZucoDataLoader()\n",
    "\n",
    "# Example: get all features\n",
    "all_features = loader.get_features()\n",
    "\n",
    "# Example: get features for a specific feature set\n",
    "electrode_features = loader.get_features(feature_set='electrode_features_all')\n",
    "\n",
    "# Example: get features for specific subjects\n",
    "selected_subjects_features = loader.get_features(\n",
    "    feature_set='sent_gaze_sacc', \n",
    "    subjects=['YAC', 'YDR']\n",
    ")\n",
    "\n",
    "# Example: get stimulus for all subjects\n",
    "all_stimulus = loader.get_stimulus()\n",
    "\n",
    "# Example: get stimulus for specific subjects and task\n",
    "specific_stimulus = loader.get_stimulus(\n",
    "    subjects=['YAC', 'YDR'], \n",
    "    task='NR'\n",
    ")\n",
    "\n",
    "# Print some details about the loaded dataset\n",
    "print(\"Available Feature Sets:\", list(loader.data['features'].keys()))\n",
    "print(\"Total Subjects:\", len(loader.metadata['subjects']))\n",
    "print(\"Channel Locations:\", loader.metadata['channel_locations'])\n",
    "print(\"Stimulus Example:\", specific_stimulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title save csv - not good - the nan's are a problem\n",
    "\n",
    "# We want to get the senteces out for an LLM, so we only need the unique sentences. \n",
    "# We should however, keep track of the sentence index and which task(s) it was used in \n",
    "\n",
    "# Create two separate DataFrames for each task\n",
    "nr_df = df2[df2['task'] == 'NR'][['index', 'sentence']].rename(columns={'index': 'NR_index'})\n",
    "tsr_df = df2[df2['task'] == 'TSR'][['index', 'sentence']].rename(columns={'index': 'TSR_index'})\n",
    "\n",
    "# Merge the DataFrames on the sentence, using outer join to keep all sentences\n",
    "result_df = pd.merge(nr_df, tsr_df, on='sentence', how='outer')\n",
    "\n",
    "# Sort by sentence for easier reading\n",
    "result_df = result_df.sort_values('sentence').reset_index(drop=True)\n",
    "\n",
    "# Print summary statistics\n",
    "nr_only = result_df[result_df['TSR_index'].isna()].shape[0]\n",
    "tsr_only = result_df[result_df['NR_index'].isna()].shape[0]\n",
    "both = result_df.dropna().shape[0]\n",
    "\n",
    "print(f\"Total unique sentences: {len(result_df)}\")\n",
    "print(f\"Sentences in NR only: {nr_only}\")\n",
    "print(f\"Sentences in TSR only: {tsr_only}\")\n",
    "print(f\"Sentences in both tasks: {both}\")\n",
    "\n",
    "# Preview the data\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(result_df.head())\n",
    "\n",
    "# Preview overlapping sentences\n",
    "print(\"\\nSample of sentences appearing in both tasks:\")\n",
    "print(result_df.dropna().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
