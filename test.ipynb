{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.load_zuco_data import *\n",
    "import numpy as np\n",
    "import torch \n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from src.load_zuco_sentences import *\n",
    "\n",
    "# tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "# transformer lens\n",
    "from transformer_lens import HookedTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD SENTENCES from ZUCO data and SAVE into a CSV \n",
    "Do not need to re-run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences into a data frame \n",
    "sentence_data_file = '../zuco-benchmark/portable_data/sentence_content.json'\n",
    "# this file has all the sentences in the dataset seen by each subject\n",
    "df_sentences = load_zuco_dataframe(sentence_data_file)\n",
    "# we don't need the subject data, we just need the sentences - which will have unique indices for a task\n",
    "df2 = df_sentences[['task', 'index', 'sentence']].drop_duplicates(subset=['task', 'index'])\n",
    "# \n",
    "unique_sentences = df2.drop_duplicates(subset=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>TSR</td>\n",
       "      <td>379</td>\n",
       "      <td>In the 40s, Gillespie led the movement called ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>TSR</td>\n",
       "      <td>380</td>\n",
       "      <td>In 1867, his brother's company, Rockefeller &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>TSR</td>\n",
       "      <td>381</td>\n",
       "      <td>Married to Almira Geraldine Goodsell, he built...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>TSR</td>\n",
       "      <td>382</td>\n",
       "      <td>Libby was a founding member of the Project for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>TSR</td>\n",
       "      <td>383</td>\n",
       "      <td>He was elected to the Bulgarian national assem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>TSR</td>\n",
       "      <td>384</td>\n",
       "      <td>He also created the Defense Intelligence Agenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>TSR</td>\n",
       "      <td>385</td>\n",
       "      <td>He was one of the founder members of the Lunar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>TSR</td>\n",
       "      <td>387</td>\n",
       "      <td>He was the founder and first president of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>TSR</td>\n",
       "      <td>388</td>\n",
       "      <td>Her mother was a Lyman, another very old Ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>TSR</td>\n",
       "      <td>389</td>\n",
       "      <td>In 1999 Bush cofounded a educational-software ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    task  index                                           sentence\n",
       "728  TSR    379  In the 40s, Gillespie led the movement called ...\n",
       "729  TSR    380  In 1867, his brother's company, Rockefeller & ...\n",
       "730  TSR    381  Married to Almira Geraldine Goodsell, he built...\n",
       "731  TSR    382  Libby was a founding member of the Project for...\n",
       "732  TSR    383  He was elected to the Bulgarian national assem...\n",
       "733  TSR    384  He also created the Defense Intelligence Agenc...\n",
       "734  TSR    385  He was one of the founder members of the Lunar...\n",
       "736  TSR    387  He was the founder and first president of the ...\n",
       "737  TSR    388  Her mother was a Lyman, another very old Ameri...\n",
       "738  TSR    389  In 1999 Bush cofounded a educational-software ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sentences.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 sentences that appear in both NR and TSR tasks:\n",
      "- Henry Ford, with his son Edsel, founded the Ford Foundation in 1936 as a local philanthropic organization with a broad charter to promote human welfare.\n",
      "- When Baldwin was young, he had a job as a busboy at famous New York City disco Studio 54.\n",
      "- Frank J. Howard (March 25, 1909 - January 26, 1996) was an American college football player and coach.\n",
      "- Talia Shire (born April 25, 1946) is an American actress of Italian descent.\n",
      "- He then enrolled at Phillips Andover, a private boarding school in Massachusetts already attended by his brother George.\n",
      "- After this initial success, Ford left Edison Illuminating and, with other investors, formed the Detroit Automobile Company.\n",
      "- He later became an educator, teaching music theory at the University of the District of Columbia; he was also director of the District of Columbia Music Center jazz workshop band.\n",
      "- She was First Lady of the United States from 1993 to 2001, as the wife of President Bill Clinton.\n",
      "- Clooney's first recordings, in May of 1946 were for Columbia Records as a singer with the big band of Tony Pastor.\n",
      "- After a career-ending injury, Howard joined the staff at Clemson College and became head coach in 1939.\n",
      "...and 46 more.\n"
     ]
    }
   ],
   "source": [
    "#let's check if there are duplicate sentences across tasks\n",
    "\n",
    "# Get set of sentences in each task\n",
    "nr_sentences = set(df2[df2['task'] == 'NR']['sentence'])\n",
    "tsr_sentences = set(df2[df2['task'] == 'TSR']['sentence'])\n",
    "\n",
    "# Find intersection\n",
    "common_sentences = nr_sentences.intersection(tsr_sentences)\n",
    "\n",
    "if common_sentences:\n",
    "    print(f\"Found {len(common_sentences)} sentences that appear in both NR and TSR tasks:\")\n",
    "    for sentence in list(common_sentences)[:10]:  # Show first 10 as example\n",
    "        print(f\"- {sentence}\")\n",
    "    if len(common_sentences) > 10:\n",
    "        print(f\"...and {len(common_sentences) - 10} more.\")\n",
    "else:\n",
    "    print(\"No sentences appear in both tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique sentences: 683\n",
      "Sentences in NR only: 291\n",
      "Sentences in TSR only: 332\n",
      "Sentences in both tasks: 60\n",
      "\n",
      "First few rows:\n",
      "   NR_index                                           sentence  TSR_index\n",
      "0       NaN  (1966), which co-starred then husband Richard ...      160.0\n",
      "1       NaN  1944, Kathleen Kennedy, known to friends as \"K...      135.0\n",
      "2       NaN  Abraham Lincoln (February 12, 1809 â€“ April 15,...      187.0\n",
      "3       NaN  Abraham Simpson is estranged husband to Mona S...      132.0\n",
      "4     114.0  According to Errol Flynn's memoirs, film direc...        NaN\n",
      "\n",
      "Sample of sentences appearing in both tasks:\n",
      "    NR_index                                           sentence  TSR_index\n",
      "6      303.0  After a career-ending injury, Howard joined th...      260.0\n",
      "10      49.0  After a two-day trial she was banished as a he...      363.0\n",
      "12     331.0  After earning his degree, Bush went to work in...      284.0\n",
      "25       1.0  After this initial success, Ford left Edison I...      356.0\n",
      "50     185.0  At the academy, he established a reputation as...       20.0\n"
     ]
    }
   ],
   "source": [
    "# We want to get the senteces out for an LLM, so we only need the unique sentences. \n",
    "# We should however, keep track of the sentence index and which task(s) it was used in \n",
    "\n",
    "# Create two separate DataFrames for each task\n",
    "nr_df = df2[df2['task'] == 'NR'][['index', 'sentence']].rename(columns={'index': 'NR_index'})\n",
    "tsr_df = df2[df2['task'] == 'TSR'][['index', 'sentence']].rename(columns={'index': 'TSR_index'})\n",
    "\n",
    "# Merge the DataFrames on the sentence, using outer join to keep all sentences\n",
    "result_df = pd.merge(nr_df, tsr_df, on='sentence', how='outer')\n",
    "\n",
    "# Sort by sentence for easier reading\n",
    "result_df = result_df.sort_values('sentence').reset_index(drop=True)\n",
    "\n",
    "# Print summary statistics\n",
    "nr_only = result_df[result_df['TSR_index'].isna()].shape[0]\n",
    "tsr_only = result_df[result_df['NR_index'].isna()].shape[0]\n",
    "both = result_df.dropna().shape[0]\n",
    "\n",
    "print(f\"Total unique sentences: {len(result_df)}\")\n",
    "print(f\"Sentences in NR only: {nr_only}\")\n",
    "print(f\"Sentences in TSR only: {tsr_only}\")\n",
    "print(f\"Sentences in both tasks: {both}\")\n",
    "\n",
    "# Preview the data\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(result_df.head())\n",
    "\n",
    "# Preview overlapping sentences\n",
    "print(\"\\nSample of sentences appearing in both tasks:\")\n",
    "print(result_df.dropna().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unique sentences with task indices to a CSV file\n",
    "csv_path = 'zuco_unique_sentences_with_task_indices.csv'\n",
    "result_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV to DATALOADER\n",
    "\n",
    "We are going to need to tokenize. For transformer lens, we can see the models which are available in the [model properties table](https://transformerlensorg.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "\n",
    "Let's start with GPT2-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "csv_path = 'zuco_unique_sentences_with_task_indices.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2-medium data loader \n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "transform = TokenizerTransform(tokenizer)\n",
    "\n",
    "dataloader = get_zuco_sentence_dataloader(\n",
    "    csv_path=csv_path,\n",
    "    transform=transform,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC (unsed/testing code - can ignore this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dataloader\n",
    "dataloader = get_zuco_sentence_dataloader(\n",
    "    csv_path=csv_path,\n",
    "    \n",
    ")\n",
    "\n",
    "# Example of iterating through the dataloader\n",
    "for batch in dataloader:\n",
    "    sentences = batch['sentence']\n",
    "    in_nr = batch['in_NR']\n",
    "    in_tsr = batch['in_TSR']\n",
    "    \n",
    "    # Your model processing here...\n",
    "    # ...\n",
    "    \n",
    "    # Just for demonstration\n",
    "    print(f\"Batch of {len(sentences)} sentences\")\n",
    "    print(f\"Number in NR task: {in_nr.sum().item()}\")\n",
    "    print(f\"Number in TSR task: {in_tsr.sum().item()}\")\n",
    "    print(f\"Sample sentence: {sentences[0]}\")\n",
    "    break  # Just show one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by task and index\n",
    "all_sentences_df = all_sentences_df.sort_values(by=['task', 'index'])\n",
    "\n",
    "# Now you can easily filter by task, subject, get unique sentences, etc.\n",
    "nr_sentences = all_sentences_df[all_sentences_df['task'] == 'NR']\n",
    "subject_tsr_sentences = all_sentences_df[(all_sentences_df['task'] == 'TSR') & \n",
    "                                        (all_sentences_df['subject'] == 'YAC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ZucoDataLoader()\n",
    "\n",
    "# Example: get all features\n",
    "all_features = loader.get_features()\n",
    "\n",
    "# Example: get features for a specific feature set\n",
    "electrode_features = loader.get_features(feature_set='electrode_features_all')\n",
    "\n",
    "# Example: get features for specific subjects\n",
    "selected_subjects_features = loader.get_features(\n",
    "    feature_set='sent_gaze_sacc', \n",
    "    subjects=['YAC', 'YDR']\n",
    ")\n",
    "\n",
    "# Example: get stimulus for all subjects\n",
    "all_stimulus = loader.get_stimulus()\n",
    "\n",
    "# Example: get stimulus for specific subjects and task\n",
    "specific_stimulus = loader.get_stimulus(\n",
    "    subjects=['YAC', 'YDR'], \n",
    "    task='NR'\n",
    ")\n",
    "\n",
    "\n",
    "# Print some details about the loaded dataset\n",
    "print(\"Available Feature Sets:\", list(loader.data['features'].keys()))\n",
    "print(\"Total Subjects:\", len(loader.metadata['subjects']))\n",
    "print(\"Channel Locations:\", loader.metadata['channel_locations'])\n",
    "print(\"Stimulus Example:\", specific_stimulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
