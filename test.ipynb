{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.load_zuco_data import *\n",
    "import numpy as np\n",
    "import torch \n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from src.load_zuco_sentences import *\n",
    "\n",
    "# tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "# transformer lens\n",
    "import tqdm.auto as tqdm\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import HookedTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD SENTENCES from ZUCO data and SAVE into a CSV \n",
    "Do not need to re-run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences into a data frame \n",
    "sentence_data_file = '../zuco-benchmark/portable_data/sentence_content.json'\n",
    "# this file has all the sentences in the dataset seen by each subject\n",
    "df_sentences = load_zuco_dataframe(sentence_data_file)\n",
    "# we don't need the subject data, we just need the sentences - which will have unique indices for a task\n",
    "df2 = df_sentences[['task', 'index', 'sentence']].drop_duplicates(subset=['task', 'index'])\n",
    "# \n",
    "unique_sentences = df2.drop_duplicates(subset=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>TSR</td>\n",
       "      <td>379</td>\n",
       "      <td>In the 40s, Gillespie led the movement called ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>TSR</td>\n",
       "      <td>380</td>\n",
       "      <td>In 1867, his brother's company, Rockefeller &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>TSR</td>\n",
       "      <td>381</td>\n",
       "      <td>Married to Almira Geraldine Goodsell, he built...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>TSR</td>\n",
       "      <td>382</td>\n",
       "      <td>Libby was a founding member of the Project for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>TSR</td>\n",
       "      <td>383</td>\n",
       "      <td>He was elected to the Bulgarian national assem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>TSR</td>\n",
       "      <td>384</td>\n",
       "      <td>He also created the Defense Intelligence Agenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>TSR</td>\n",
       "      <td>385</td>\n",
       "      <td>He was one of the founder members of the Lunar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>TSR</td>\n",
       "      <td>387</td>\n",
       "      <td>He was the founder and first president of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>TSR</td>\n",
       "      <td>388</td>\n",
       "      <td>Her mother was a Lyman, another very old Ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>TSR</td>\n",
       "      <td>389</td>\n",
       "      <td>In 1999 Bush cofounded a educational-software ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    task  index                                           sentence\n",
       "728  TSR    379  In the 40s, Gillespie led the movement called ...\n",
       "729  TSR    380  In 1867, his brother's company, Rockefeller & ...\n",
       "730  TSR    381  Married to Almira Geraldine Goodsell, he built...\n",
       "731  TSR    382  Libby was a founding member of the Project for...\n",
       "732  TSR    383  He was elected to the Bulgarian national assem...\n",
       "733  TSR    384  He also created the Defense Intelligence Agenc...\n",
       "734  TSR    385  He was one of the founder members of the Lunar...\n",
       "736  TSR    387  He was the founder and first president of the ...\n",
       "737  TSR    388  Her mother was a Lyman, another very old Ameri...\n",
       "738  TSR    389  In 1999 Bush cofounded a educational-software ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sentences.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 sentences that appear in both NR and TSR tasks:\n",
      "- In 1966 she went to United Artists Records.\n",
      "- He served in the United States Army in World War II, then got a law degree from Tulane University.\n",
      "- After a two-day trial she was banished as a heretic in 1638 and led 60 followers to settle Aquidneck Island in what later became Rhode Island.\n",
      "- Erasmus Darwin (December 12, 1731 â€“ April 18, 1802) trained as a physician and wrote extensively on medicine and botany, as well as poetry.\n",
      "- That year, he also married Rose Fitzgerald, the daughter of John F. Fitzgerald, the Democrat mayor of Boston and probably the most recognized politician in the city.\n",
      "- When he went to the nearby Shrewsbury School the next year, he lived there as a 'boarder'.\n",
      "- She was First Lady of the United States from 1993 to 2001, as the wife of President Bill Clinton.\n",
      "- He became a banker in Boston and was active in the local Democratic Party.\n",
      "- It was published in 1950 and earned him some respect as a writer.\n",
      "- Frank J. Howard (March 25, 1909 - January 26, 1996) was an American college football player and coach.\n",
      "...and 46 more.\n"
     ]
    }
   ],
   "source": [
    "#let's check if there are duplicate sentences across tasks\n",
    "\n",
    "# Get set of sentences in each task\n",
    "nr_sentences = set(df2[df2['task'] == 'NR']['sentence'])\n",
    "tsr_sentences = set(df2[df2['task'] == 'TSR']['sentence'])\n",
    "\n",
    "# Find intersection\n",
    "common_sentences = nr_sentences.intersection(tsr_sentences)\n",
    "\n",
    "if common_sentences:\n",
    "    print(f\"Found {len(common_sentences)} sentences that appear in both NR and TSR tasks:\")\n",
    "    for sentence in list(common_sentences)[:10]:  # Show first 10 as example\n",
    "        print(f\"- {sentence}\")\n",
    "    if len(common_sentences) > 10:\n",
    "        print(f\"...and {len(common_sentences) - 10} more.\")\n",
    "else:\n",
    "    print(\"No sentences appear in both tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title: save csv (note that index = -100 if the sentence is not present in one of the tasks)\n",
    "\n",
    "# We want to get the senteces out for an LLM, so we only need the unique sentences. \n",
    "# We should however, keep track of the sentence index and which task(s) it was used in \n",
    "# We can do this by merging the two dataframes on the sentence column, and keeping the index from each task\n",
    "# We will also fill in the index with -100 for sentences that are not present in one of the tasks (in order to avoid nans)\n",
    "\n",
    "\n",
    "# Create two separate DataFrames for each task, dropping duplicates first\n",
    "nr_df = df2[df2['task'] == 'NR'][['index', 'sentence']].drop_duplicates(subset=['sentence']).rename(columns={'index': 'NR_index'})\n",
    "tsr_df = df2[df2['task'] == 'TSR'][['index', 'sentence']].drop_duplicates(subset=['sentence']).rename(columns={'index': 'TSR_index'})\n",
    "\n",
    "# Merge the DataFrames on the sentence, using outer join to keep all sentences\n",
    "result_df = pd.merge(nr_df, tsr_df, on='sentence', how='outer')\n",
    "\n",
    "# Fill NaN values with -100\n",
    "result_df = result_df.fillna(-100)\n",
    "\n",
    "# Convert indices to integers\n",
    "result_df['NR_index'] = result_df['NR_index'].astype(int)\n",
    "result_df['TSR_index'] = result_df['TSR_index'].astype(int)\n",
    "\n",
    "# Sort by sentence for easier reading\n",
    "result_df = result_df.sort_values('sentence').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unique sentences with task indices to a CSV file\n",
    "csv_path = 'zuco_unique_sentences_with_task_indices.csv'\n",
    "result_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV to DATALOADER\n",
    "\n",
    "We are going to need to tokenize. For transformer lens, we can see the models which are available in the [model properties table](https://transformerlensorg.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "\n",
    "Let's start with GPT2-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title: csv file and MODEL\n",
    "csv_path = 'zuco_unique_sentences_with_task_indices.csv'\n",
    "model_name = 'gpt2-medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2-medium data loader \n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "transform = TokenizerTransform(tokenizer) \n",
    "\n",
    "dataloader = get_zuco_sentence_dataloader(\n",
    "    csv_path=csv_path,\n",
    "    transform=transform,\n",
    "    batch_size=15,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of 15 sentences\n",
      "Number in NR task: 6\n",
      "NR indices: tensor([ 260, -100,   64, -100, -100, -100, -100, -100,  134,  153,  128, -100,\n",
      "         252, -100, -100])\n",
      "Number in TSR task: 9\n",
      "TSR indices: tensor([-100,  179, -100,  204,  385,  166,  189,  232, -100, -100, -100,    5,\n",
      "        -100,    7,  226])\n",
      "Sample sentence: He was campaign manager for Ford's 1976 presidential campaign, while James Baker served as campaign chairman.\n"
     ]
    }
   ],
   "source": [
    "# testing out the dataloader\n",
    "\n",
    "\n",
    "# Example of iterating through the dataloader\n",
    "for batch in dataloader:\n",
    "    sentences = batch['sentence']\n",
    "    in_nr = batch['in_NR']\n",
    "    in_tsr = batch['in_TSR']\n",
    "    NR_indices = batch['NR_index']\n",
    "    TSR_indices = batch['TSR_index']\n",
    "    \n",
    "   # Your model processing here...\n",
    "    # ...\n",
    "    \n",
    "    # Just for demonstration\n",
    "    print(f\"Batch of {len(sentences)} sentences\")\n",
    "    print(f\"Number in NR task: {in_nr.sum().item()}\")\n",
    "    print(f\"NR indices: {NR_indices}\")\n",
    "    print(f\"Number in TSR task: {in_tsr.sum().item()}\")\n",
    "    print(f\"TSR indices: {TSR_indices}\")\n",
    "\n",
    "    print(f\"Sample sentence: {sentences[0]}\")\n",
    "    break  # Just show one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2-medium with transformer lens- collect activations\n",
    "For Transformer-lens demos, see their [github page](https://github.com/TransformerLensOrg/TransformerLens/tree/main/demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x15f78cf10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False) # don't need gradients for this (not training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# load model \n",
    "model = HookedTransformer.from_pretrained(model_name) # note that model name is defined above with the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: gpt2-medium\n",
      "Number of layers: 24\n",
      "Number of heads: 16\n",
      "Context length: 1024\n",
      "Hidden size: 1024\n",
      "MLP intermediate size: 4096\n",
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Basic model information\n",
    "print(f\"Model name: {model.cfg.model_name}\")\n",
    "print(f\"Number of layers: {model.cfg.n_layers}\")\n",
    "print(f\"Number of heads: {model.cfg.n_heads}\")\n",
    "print(f\"Context length: {model.cfg.n_ctx}\")\n",
    "print(f\"Hidden size: {model.cfg.d_model}\")\n",
    "print(f\"MLP intermediate size: {model.cfg.d_mlp}\")\n",
    "print(f\"Vocabulary size: {model.cfg.d_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from TransformerLens/demos/Interactive_Neuroscope.ipynb\n",
    "def get_neuron_acts(model, text, layer, neuron_index=None):\n",
    "    # Hacky way to get out state from a single hook - we have a single element list and edit that list within the hook.\n",
    "    cache = {}\n",
    "\n",
    "    def caching_hook(act, hook):\n",
    "        if neuron_index:\n",
    "            cache[\"activation\"] = act[:, :, neuron_index]\n",
    "        else: \n",
    "            cache[\"activation\"] = act\n",
    "\n",
    "    model.run_with_hooks(\n",
    "        text, fwd_hooks=[(f\"blocks.{layer}.mlp.hook_post\", caching_hook)]\n",
    "    )\n",
    "    return utils.to_numpy(cache[\"activation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC (unused/testing code - can ignore this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_running_the_notebook' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mstop_running_the_notebook\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'stop_running_the_notebook' is not defined"
     ]
    }
   ],
   "source": [
    "stop_running_the_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by task and index\n",
    "all_sentences_df = all_sentences_df.sort_values(by=['task', 'index'])\n",
    "\n",
    "# Now you can easily filter by task, subject, get unique sentences, etc.\n",
    "nr_sentences = all_sentences_df[all_sentences_df['task'] == 'NR']\n",
    "subject_tsr_sentences = all_sentences_df[(all_sentences_df['task'] == 'TSR') & \n",
    "                                        (all_sentences_df['subject'] == 'YAC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ZucoDataLoader()\n",
    "\n",
    "# Example: get all features\n",
    "all_features = loader.get_features()\n",
    "\n",
    "# Example: get features for a specific feature set\n",
    "electrode_features = loader.get_features(feature_set='electrode_features_all')\n",
    "\n",
    "# Example: get features for specific subjects\n",
    "selected_subjects_features = loader.get_features(\n",
    "    feature_set='sent_gaze_sacc', \n",
    "    subjects=['YAC', 'YDR']\n",
    ")\n",
    "\n",
    "# Example: get stimulus for all subjects\n",
    "all_stimulus = loader.get_stimulus()\n",
    "\n",
    "# Example: get stimulus for specific subjects and task\n",
    "specific_stimulus = loader.get_stimulus(\n",
    "    subjects=['YAC', 'YDR'], \n",
    "    task='NR'\n",
    ")\n",
    "\n",
    "# Print some details about the loaded dataset\n",
    "print(\"Available Feature Sets:\", list(loader.data['features'].keys()))\n",
    "print(\"Total Subjects:\", len(loader.metadata['subjects']))\n",
    "print(\"Channel Locations:\", loader.metadata['channel_locations'])\n",
    "print(\"Stimulus Example:\", specific_stimulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title save csv - not good - the nan's are a problem\n",
    "\n",
    "# We want to get the senteces out for an LLM, so we only need the unique sentences. \n",
    "# We should however, keep track of the sentence index and which task(s) it was used in \n",
    "\n",
    "# Create two separate DataFrames for each task\n",
    "nr_df = df2[df2['task'] == 'NR'][['index', 'sentence']].rename(columns={'index': 'NR_index'})\n",
    "tsr_df = df2[df2['task'] == 'TSR'][['index', 'sentence']].rename(columns={'index': 'TSR_index'})\n",
    "\n",
    "# Merge the DataFrames on the sentence, using outer join to keep all sentences\n",
    "result_df = pd.merge(nr_df, tsr_df, on='sentence', how='outer')\n",
    "\n",
    "# Sort by sentence for easier reading\n",
    "result_df = result_df.sort_values('sentence').reset_index(drop=True)\n",
    "\n",
    "# Print summary statistics\n",
    "nr_only = result_df[result_df['TSR_index'].isna()].shape[0]\n",
    "tsr_only = result_df[result_df['NR_index'].isna()].shape[0]\n",
    "both = result_df.dropna().shape[0]\n",
    "\n",
    "print(f\"Total unique sentences: {len(result_df)}\")\n",
    "print(f\"Sentences in NR only: {nr_only}\")\n",
    "print(f\"Sentences in TSR only: {tsr_only}\")\n",
    "print(f\"Sentences in both tasks: {both}\")\n",
    "\n",
    "# Preview the data\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(result_df.head())\n",
    "\n",
    "# Preview overlapping sentences\n",
    "print(\"\\nSample of sentences appearing in both tasks:\")\n",
    "print(result_df.dropna().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
