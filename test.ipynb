{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_zuco_data import *\n",
    "import numpy as np\n",
    "import torch \n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from src.load_zuco_sentences import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD SENTENCES from ZUCO data and SAVE into a CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences into a data frame \n",
    "sentence_data_file = '../zuco-benchmark/portable_data/sentence_content.json'\n",
    "# this file has all the sentences in the dataset seen by each subject\n",
    "df_sentences = load_zuco_dataframe(sentence_data_file)\n",
    "# we don't need the subject data, we just need the sentences - which will have unique indices for a task\n",
    "df2 = df_sentences[['task', 'index', 'sentence']].drop_duplicates(subset=['task', 'index'])\n",
    "# \n",
    "unique_sentences = df2.drop_duplicates(subset=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>TSR</td>\n",
       "      <td>379</td>\n",
       "      <td>In the 40s, Gillespie led the movement called ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>TSR</td>\n",
       "      <td>380</td>\n",
       "      <td>In 1867, his brother's company, Rockefeller &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>TSR</td>\n",
       "      <td>381</td>\n",
       "      <td>Married to Almira Geraldine Goodsell, he built...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>TSR</td>\n",
       "      <td>382</td>\n",
       "      <td>Libby was a founding member of the Project for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>TSR</td>\n",
       "      <td>383</td>\n",
       "      <td>He was elected to the Bulgarian national assem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>TSR</td>\n",
       "      <td>384</td>\n",
       "      <td>He also created the Defense Intelligence Agenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>TSR</td>\n",
       "      <td>385</td>\n",
       "      <td>He was one of the founder members of the Lunar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>TSR</td>\n",
       "      <td>387</td>\n",
       "      <td>He was the founder and first president of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>TSR</td>\n",
       "      <td>388</td>\n",
       "      <td>Her mother was a Lyman, another very old Ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>TSR</td>\n",
       "      <td>389</td>\n",
       "      <td>In 1999 Bush cofounded a educational-software ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    task  index                                           sentence\n",
       "728  TSR    379  In the 40s, Gillespie led the movement called ...\n",
       "729  TSR    380  In 1867, his brother's company, Rockefeller & ...\n",
       "730  TSR    381  Married to Almira Geraldine Goodsell, he built...\n",
       "731  TSR    382  Libby was a founding member of the Project for...\n",
       "732  TSR    383  He was elected to the Bulgarian national assem...\n",
       "733  TSR    384  He also created the Defense Intelligence Agenc...\n",
       "734  TSR    385  He was one of the founder members of the Lunar...\n",
       "736  TSR    387  He was the founder and first president of the ...\n",
       "737  TSR    388  Her mother was a Lyman, another very old Ameri...\n",
       "738  TSR    389  In 1999 Bush cofounded a educational-software ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sentences.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 sentences that appear in both NR and TSR tasks:\n",
      "- Henry Ford, with his son Edsel, founded the Ford Foundation in 1936 as a local philanthropic organization with a broad charter to promote human welfare.\n",
      "- When Baldwin was young, he had a job as a busboy at famous New York City disco Studio 54.\n",
      "- Frank J. Howard (March 25, 1909 - January 26, 1996) was an American college football player and coach.\n",
      "- Talia Shire (born April 25, 1946) is an American actress of Italian descent.\n",
      "- He then enrolled at Phillips Andover, a private boarding school in Massachusetts already attended by his brother George.\n",
      "- After this initial success, Ford left Edison Illuminating and, with other investors, formed the Detroit Automobile Company.\n",
      "- He later became an educator, teaching music theory at the University of the District of Columbia; he was also director of the District of Columbia Music Center jazz workshop band.\n",
      "- She was First Lady of the United States from 1993 to 2001, as the wife of President Bill Clinton.\n",
      "- Clooney's first recordings, in May of 1946 were for Columbia Records as a singer with the big band of Tony Pastor.\n",
      "- After a career-ending injury, Howard joined the staff at Clemson College and became head coach in 1939.\n",
      "...and 46 more.\n"
     ]
    }
   ],
   "source": [
    "#let's check if there are duplicate sentences across tasks\n",
    "\n",
    "# Get set of sentences in each task\n",
    "nr_sentences = set(df2[df2['task'] == 'NR']['sentence'])\n",
    "tsr_sentences = set(df2[df2['task'] == 'TSR']['sentence'])\n",
    "\n",
    "# Find intersection\n",
    "common_sentences = nr_sentences.intersection(tsr_sentences)\n",
    "\n",
    "if common_sentences:\n",
    "    print(f\"Found {len(common_sentences)} sentences that appear in both NR and TSR tasks:\")\n",
    "    for sentence in list(common_sentences)[:10]:  # Show first 10 as example\n",
    "        print(f\"- {sentence}\")\n",
    "    if len(common_sentences) > 10:\n",
    "        print(f\"...and {len(common_sentences) - 10} more.\")\n",
    "else:\n",
    "    print(\"No sentences appear in both tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique sentences: 683\n",
      "Sentences in NR only: 291\n",
      "Sentences in TSR only: 332\n",
      "Sentences in both tasks: 60\n",
      "\n",
      "First few rows:\n",
      "   NR_index                                           sentence  TSR_index\n",
      "0       NaN  (1966), which co-starred then husband Richard ...      160.0\n",
      "1       NaN  1944, Kathleen Kennedy, known to friends as \"K...      135.0\n",
      "2       NaN  Abraham Lincoln (February 12, 1809 â€“ April 15,...      187.0\n",
      "3       NaN  Abraham Simpson is estranged husband to Mona S...      132.0\n",
      "4     114.0  According to Errol Flynn's memoirs, film direc...        NaN\n",
      "\n",
      "Sample of sentences appearing in both tasks:\n",
      "    NR_index                                           sentence  TSR_index\n",
      "6      303.0  After a career-ending injury, Howard joined th...      260.0\n",
      "10      49.0  After a two-day trial she was banished as a he...      363.0\n",
      "12     331.0  After earning his degree, Bush went to work in...      284.0\n",
      "25       1.0  After this initial success, Ford left Edison I...      356.0\n",
      "50     185.0  At the academy, he established a reputation as...       20.0\n"
     ]
    }
   ],
   "source": [
    "# We want to get the senteces out for an LLM, so we only need the unique sentences. \n",
    "# We should however, keep track of the sentence index and which task(s) it was used in \n",
    "\n",
    "# Create two separate DataFrames for each task\n",
    "nr_df = df2[df2['task'] == 'NR'][['index', 'sentence']].rename(columns={'index': 'NR_index'})\n",
    "tsr_df = df2[df2['task'] == 'TSR'][['index', 'sentence']].rename(columns={'index': 'TSR_index'})\n",
    "\n",
    "# Merge the DataFrames on the sentence, using outer join to keep all sentences\n",
    "result_df = pd.merge(nr_df, tsr_df, on='sentence', how='outer')\n",
    "\n",
    "# Sort by sentence for easier reading\n",
    "result_df = result_df.sort_values('sentence').reset_index(drop=True)\n",
    "\n",
    "# Print summary statistics\n",
    "nr_only = result_df[result_df['TSR_index'].isna()].shape[0]\n",
    "tsr_only = result_df[result_df['NR_index'].isna()].shape[0]\n",
    "both = result_df.dropna().shape[0]\n",
    "\n",
    "print(f\"Total unique sentences: {len(result_df)}\")\n",
    "print(f\"Sentences in NR only: {nr_only}\")\n",
    "print(f\"Sentences in TSR only: {tsr_only}\")\n",
    "print(f\"Sentences in both tasks: {both}\")\n",
    "\n",
    "# Preview the data\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(result_df.head())\n",
    "\n",
    "# Preview overlapping sentences\n",
    "print(\"\\nSample of sentences appearing in both tasks:\")\n",
    "print(result_df.dropna().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unique sentences with task indices to a CSV file\n",
    "csv_path = 'zuco_unique_sentences_with_task_indices.csv'\n",
    "result_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV to DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "csv_path = 'zuco_unique_sentences_with_task_indices.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example with a pretend tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "transform = TokenizerTransform(tokenizer)\n",
    "\n",
    "dataloader = get_zuco_sentence_dataloader(\n",
    "    csv_path=csv_path,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ~~~~~~~~~~~~~~~^^^^^^\n  File \"/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 172, in collate\n    key: collate(\n         ~~~~~~~^\n        [d[key] for d in batch], collate_fn_map=collate_fn_map\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 159, in collate\n    return collate_fn_map[collate_type](\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        batch, collate_fn_map=collate_fn_map\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 293, in collate_numpy_scalar_fn\n    return torch.as_tensor(batch)\n           ~~~~~~~~~~~~~~~^^^^^^^\nRuntimeError: Could not infer dtype of NoneType\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m dataloader = get_zuco_sentence_dataloader(\n\u001b[32m      3\u001b[39m     csv_path=csv_path,\n\u001b[32m      4\u001b[39m     batch_size=\u001b[32m16\u001b[39m,\n\u001b[32m      5\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Example of iterating through the dataloader\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentence\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_nr\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43min_NR\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1480\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1478\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task_info[idx]\n\u001b[32m   1479\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1480\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1505\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1503\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1504\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1505\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/_utils.py:733\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ~~~~~~~~~~~~~~~^^^^^^\n  File \"/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 172, in collate\n    key: collate(\n         ~~~~~~~^\n        [d[key] for d in batch], collate_fn_map=collate_fn_map\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 159, in collate\n    return collate_fn_map[collate_type](\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        batch, collate_fn_map=collate_fn_map\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/oshun/Documents/GitHub/AISC2024_Zucco2LLM/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 293, in collate_numpy_scalar_fn\n    return torch.as_tensor(batch)\n           ~~~~~~~~~~~~~~~^^^^^^^\nRuntimeError: Could not infer dtype of NoneType\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a dataloader\n",
    "dataloader = get_zuco_sentence_dataloader(\n",
    "    csv_path=csv_path,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Example of iterating through the dataloader\n",
    "for batch in dataloader:\n",
    "    sentences = batch['sentence']\n",
    "    in_nr = batch['in_NR']\n",
    "    in_tsr = batch['in_TSR']\n",
    "    \n",
    "    # Your model processing here...\n",
    "    # ...\n",
    "    \n",
    "    # Just for demonstration\n",
    "    print(f\"Batch of {len(sentences)} sentences\")\n",
    "    print(f\"Number in NR task: {in_nr.sum().item()}\")\n",
    "    print(f\"Number in TSR task: {in_tsr.sum().item()}\")\n",
    "    print(f\"Sample sentence: {sentences[0]}\")\n",
    "    break  # Just show one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by task and index\n",
    "all_sentences_df = all_sentences_df.sort_values(by=['task', 'index'])\n",
    "\n",
    "# Now you can easily filter by task, subject, get unique sentences, etc.\n",
    "nr_sentences = all_sentences_df[all_sentences_df['task'] == 'NR']\n",
    "subject_tsr_sentences = all_sentences_df[(all_sentences_df['task'] == 'TSR') & \n",
    "                                        (all_sentences_df['subject'] == 'YAC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ZucoDataLoader()\n",
    "\n",
    "# Example: get all features\n",
    "all_features = loader.get_features()\n",
    "\n",
    "# Example: get features for a specific feature set\n",
    "electrode_features = loader.get_features(feature_set='electrode_features_all')\n",
    "\n",
    "# Example: get features for specific subjects\n",
    "selected_subjects_features = loader.get_features(\n",
    "    feature_set='sent_gaze_sacc', \n",
    "    subjects=['YAC', 'YDR']\n",
    ")\n",
    "\n",
    "# Example: get stimulus for all subjects\n",
    "all_stimulus = loader.get_stimulus()\n",
    "\n",
    "# Example: get stimulus for specific subjects and task\n",
    "specific_stimulus = loader.get_stimulus(\n",
    "    subjects=['YAC', 'YDR'], \n",
    "    task='NR'\n",
    ")\n",
    "\n",
    "\n",
    "# Print some details about the loaded dataset\n",
    "print(\"Available Feature Sets:\", list(loader.data['features'].keys()))\n",
    "print(\"Total Subjects:\", len(loader.metadata['subjects']))\n",
    "print(\"Channel Locations:\", loader.metadata['channel_locations'])\n",
    "print(\"Stimulus Example:\", specific_stimulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
